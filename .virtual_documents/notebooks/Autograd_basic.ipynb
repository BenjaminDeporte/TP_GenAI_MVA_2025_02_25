





import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import time

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


torch.set_default_dtype(torch.float64)

if torch.cuda.is_available():
    device = torch.device('cuda')
    dtype = torch.cuda.FloatTensor
else:
    device = torch.device('cpu')
    dtype = torch.FloatTensor

# If you don't want to bother with the device, stay on cpu:
# device = torch.device('cpu')

print(f"Using {device}")

!nvidia-smi

# set seeds for reproductibility
random_seed = 42
rng = np.random.default_rng(seed=random_seed)
torch.manual_seed(random_seed)





a = torch.tensor([2., 3.], requires_grad=True)
b = torch.tensor([6., 4.], requires_grad=True)

print(f"a : {a.size()}")
print(f"b : {b.size()}")


q = 3*a**3 + 2*b**2

print(f"q = {q}")
print(f"q : {q.size()}")





external_grad = torch.tensor([1., 1.])


q.backward(gradient=external_grad)





a.grad





b.grad








n_samples = 5


X = torch.randn((n_samples), requires_grad=True)
W = torch.tensor([2.0], requires_grad=True)

print(f"X : {X.size()}")
print(f"W : {W.size()}")


Y = W * X
Y.size()





external_grad = torch.ones(Y.size()[0])
external_grad.size()


g = torch.autograd.grad(outputs=Y, inputs=W, grad_outputs=external_grad, retain_graph=True)


g[0].size()


g[0]





X.sum()





external_grad = torch.ones(Y.size()[0])
external_grad.size()


h = torch.autograd.grad(outputs=Y, inputs=X, grad_outputs=external_grad)


h[0].size()


h[0]








n_samples = 5
n_dim = 3

X = torch.randn((n_samples, n_dim), requires_grad=True)


X


W = torch.tensor([1.0,2.0,3.0])


Y = X @ W
print(Y)


external_gradients = torch.ones(Y.size()[0])
external_gradients





j = torch.autograd.grad(outputs=Y, inputs=X, grad_outputs=external_gradients,retain_graph=True)


j[0]


external_gradients=torch.tensor([0.0,1.0,2.0,3.0,-1.0])


k = torch.autograd.grad(outputs=Y, inputs=X, grad_outputs=external_gradients)


k[0]
